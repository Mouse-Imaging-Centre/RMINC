% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/minc_parallel.R
\name{pMincApply}
\alias{pMincApply}
\title{Parallel MincApply}
\usage{
pMincApply(filenames, fun, ..., mask = NULL, tinyMask = FALSE,
  batches = 4, slab_sizes = NULL, method = NULL, local = FALSE,
  cores = NULL, resources = NULL, packages = NULL, vmem = NULL,
  walltime = NULL, workers = batches, temp_dir = getwd(),
  cleanup = TRUE, collate = simplify2minc,
  conf_file = getOption("RMINC_BATCH_CONF"),
  registry_name = new_file("pMincApply_registry"), registry_dir = getwd())
}
\arguments{
\item{filenames}{Paths to the minc files to be applied accross}

\item{fun}{The function to apply}

\item{...}{Additional arguments to fun through \link{mincApplyRCPP} see 
notes for a warnings}

\item{mask}{The path to a mask for the minc files}

\item{tinyMask}{whether to use a small subset of voxels to test the computation}

\item{batches}{The number of jobs to break the computation into, ignored for
snowfall/local mode}

\item{slab_sizes}{A 3 element vector indicating how large a chunk of data to read from each minc file 
at a time defaults to one slice along the first dimension.}

\item{method}{A deprecated argument formerly used to configure how to parallelize the jobs
now this is handled with \code{conf_file}}

\item{local}{boolean whether to run the jobs locally (with the parallel package) or with batchtools}

\item{cores}{defaults to 1 or
\code{max(getOption("mc.cores"), parallel::detectCores() - 1)} if running locally
see \link{qMincApply} for details.}

\item{resources}{A list of resources to request from the queueing system
common examples including vmem, walltime, nodes, and modules see
\code{system.file("parallel/pbs_script.tmpl", package = "RMINC")} and
\code{system.file("parallel/sge_script.tmpl", package = "RMINC")} for
more details}

\item{packages}{Character vector of packages to load for all jobs}

\item{vmem}{The number of gigabytes of memory to request for each batched
job. It is a compatibility argument and will overload \code{vmem} 
set in the resource list (if it is defined)}

\item{walltime}{The amount of walltime to request for each batched job.
It is a compatibility argument and will overwrite \code{walltime}
set in the resource list (if it is defined)}

\item{workers}{The number of workers to use. It is a compatibility option
and will overwrite \code{batches} if it is supplied.}

\item{temp_dir}{A path to a temporary directory to hold the job registry
created when using a true queuing system and for writing temporary mask
files. This must be a location read/writable by all nodes when using a
true queuing system (so /tmp will not work).}

\item{cleanup}{Whether to clean up registry after a queue parallelization job}

\item{collate}{A function to be applied to collapse the results of the
the pMincApply. Defaults to \link{simplify2minc}.}

\item{conf_file}{A batchtools config file, defaults to \code{option("RMINC_BATCH_CONF")}}

\item{registry_name}{a name for the registry}

\item{registry_dir}{where batchtools should create the registry}
}
\value{
The results of applying \code{fun} to each voxel accross \code{filenames}
after collation with \code{collate}
}
\description{
Apply an arbitrary R function across a collection of minc files, distributing
the computation to multiple cores or workers on a grid computing environment
}
\details{
This is a convenience wrapper for two underlying functions \link{qMincApply}
and \link{mcMincApply} for queueing and multicore processing respectively. Each of
these functions divides all of the voxels that are masked by \code{mask} into
\code{batches}. Batches are processed in parallel, with calling
\link{mincApplyRCPP} to process the voxels in the batch. Arguments passed in through
\code{...} will be bound by \link{mincApplyRCPP} before \code{fun}, so be wary
of potential partial matches. When in doubt, partially apply your function before
hand, and do not rely on positional matching.
}
